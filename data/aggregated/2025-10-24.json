[
  {
    "arxiv_id": "2510.20818v1",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for   Capability-Modulated and Steerable Navigation",
    "summary": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
    "authors": [
      "Mateo Guaman Castro",
      "Sidharth Rajagopal",
      "Daniel Gorbatov",
      "Matt Schmittle",
      "Rohan Baijal",
      "Octi Zhang",
      "Rosario Scalise",
      "Sidharth Talia",
      "Emma Romig",
      "Celso de Melo",
      "Byron Boots",
      "Abhishek Gupta"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20818v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20818v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.88
  },
  {
    "arxiv_id": "2510.20820v1",
    "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered   Canvas",
    "summary": "Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.",
    "authors": [
      "Guocheng Gordon Qian",
      "Ruihang Zhang",
      "Tsai-Shien Chen",
      "Yusuf Dalva",
      "Anujraaj Argo Goyal",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Meng Dong",
      "Arpit Sahni",
      "Daniil Ostashev",
      "Ju Hu",
      "Sergey Tulyakov",
      "Kuan-Chieh Jackson Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20820v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20820v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20819v1",
    "title": "Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge",
    "summary": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
    "authors": [
      "Nimrod Berman",
      "Omkar Joglekar",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20819v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20819v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2510.20822v1",
    "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video   Narratives",
    "summary": "State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.",
    "authors": [
      "Yihao Meng",
      "Hao Ouyang",
      "Yue Yu",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Hanlin Wang",
      "Yixuan Li",
      "Cheng Chen",
      "Yanhong Zeng",
      "Yujun Shen",
      "Huamin Qu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20822v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20822v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2510.20812v1",
    "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation",
    "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict",
    "authors": [
      "Yuhan Liu",
      "Lianhui Qin",
      "Shengjie Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20812v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20812v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2510.20817v1",
    "title": "KL-Regularized Reinforcement Learning is Designed to Mode Collapse",
    "summary": "It is commonly believed that optimizing the reverse KL divergence results in \"mode seeking\", while optimizing forward KL results in \"mass covering\", with the latter being preferred if the goal is to sample from multiple diverse modes. We show -- mathematically and empirically -- that this intuition does not necessarily transfer well to doing reinforcement learning with reverse/forward KL regularization (e.g. as commonly used with language models). Instead, the choice of reverse/forward KL determines the family of optimal target distributions, parameterized by the regularization coefficient. Mode coverage depends primarily on other factors, such as regularization strength, and relative scales between rewards and reference probabilities. Further, we show commonly used settings such as low regularization strength and equal verifiable rewards tend to specify unimodal target distributions, meaning the optimization objective is, by construction, non-diverse. We leverage these insights to construct a simple, scalable, and theoretically justified algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a target distribution which puts high probability over all high-quality sampling modes. In experiments, this simple modification works to post-train both Large Language Models and Chemical Language Models to have higher solution quality and diversity, without any external signals of diversity, and works with both forward and reverse KL when using either naively fails.",
    "authors": [
      "Anthony GX-Chen",
      "Jatin Prakash",
      "Jeff Guo",
      "Rob Fergus",
      "Rajesh Ranganath"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20817v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20817v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2510.20809v1",
    "title": "Real Deep Research for AI, Robotics and Beyond",
    "summary": "With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.",
    "authors": [
      "Xueyan Zou",
      "Jianglong Ye",
      "Hao Zhang",
      "Xiaoyu Xiang",
      "Mingyu Ding",
      "Zhaojing Yang",
      "Yong Jae Lee",
      "Zhuowen Tu",
      "Sifei Liu",
      "Xiaolong Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20809v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20809v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2510.20813v1",
    "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic   Manipulation",
    "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
    "authors": [
      "Guangqi Jiang",
      "Haoran Chang",
      "Ri-Zhao Qiu",
      "Yutong Liang",
      "Mazeyu Ji",
      "Jiyue Zhu",
      "Zhao Dong",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20813v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20813v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2510.20810v1",
    "title": "On the Detectability of LLM-Generated Text: What Exactly Is   LLM-Generated Text?",
    "summary": "With the widespread use of large language models (LLMs), many researchers have turned their attention to detecting text generated by them. However, there is no consistent or precise definition of their target, namely \"LLM-generated text\". Differences in usage scenarios and the diversity of LLMs further increase the difficulty of detection. What is commonly regarded as the detecting target usually represents only a subset of the text that LLMs can potentially produce. Human edits to LLM outputs, together with the subtle influences that LLMs exert on their users, are blurring the line between LLM-generated and human-written text. Existing benchmarks and evaluation approaches do not adequately address the various conditions in real-world detector applications. Hence, the numerical results of detectors are often misunderstood, and their significance is diminishing. Therefore, detectors remain useful under specific conditions, but their results should be interpreted only as references rather than decisive indicators.",
    "authors": [
      "Mingmeng Geng",
      "Thierry Poibeau"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20810v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20810v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "model_id": "raomnb/SN381",
    "author": "unknown",
    "title": "raomnb/SN381",
    "downloads": 16134,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T03:21:04.000Z",
    "last_modified": "2025-10-24T18:16:57.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/raomnb/SN381",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "model_id": "sohayeb/ModernBERT-base-finetuned-ag-news",
    "author": "unknown",
    "title": "sohayeb/ModernBERT-base-finetuned-ag-news",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "modernbert",
      "text-classification",
      "generated_from_trainer",
      "base_model:answerdotai/ModernBERT-base",
      "base_model:finetune:answerdotai/ModernBERT-base",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "library": "transformers",
    "created_at": "2025-10-24T04:46:05.000Z",
    "last_modified": "2025-10-24T18:16:52.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/sohayeb/ModernBERT-base-finetuned-ag-news",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "model_id": "AussieAck/D16_model",
    "author": "unknown",
    "title": "AussieAck/D16_model",
    "downloads": 10945,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-22T19:06:26.000Z",
    "last_modified": "2025-10-24T18:16:46.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/AussieAck/D16_model",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "model_id": "kisonka80/box",
    "author": "unknown",
    "title": "kisonka80/box",
    "downloads": 10521,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-23T15:35:51.000Z",
    "last_modified": "2025-10-24T18:16:38.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/kisonka80/box",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "model_id": "Makeena-2025/D14_model",
    "author": "unknown",
    "title": "Makeena-2025/D14_model",
    "downloads": 10290,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-23T20:54:19.000Z",
    "last_modified": "2025-10-24T18:16:32.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Makeena-2025/D14_model",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "model_id": "AussieAck/D26_model",
    "author": "unknown",
    "title": "AussieAck/D26_model",
    "downloads": 11520,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-23T20:56:38.000Z",
    "last_modified": "2025-10-24T18:15:43.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/AussieAck/D26_model",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2510.20814v1",
    "title": "SpectraMorph: Structured Latent Learning for Self-Supervised   Hyperspectral Super-Resolution",
    "summary": "Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.",
    "authors": [
      "Ritik Shah",
      "Marco F Duarte"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-10-23",
    "url": "https://arxiv.org/abs/2510.20814v1",
    "pdf_url": "https://arxiv.org/pdf/2510.20814v1.pdf",
    "date": "2025-10-24",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "model_id": "naitikrishu/news-category-model",
    "author": "unknown",
    "title": "naitikrishu/news-category-model",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T18:18:11.000Z",
    "last_modified": "2025-10-24T18:18:11.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/naitikrishu/news-category-model",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "author": "unknown",
    "title": "pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "downloads": 522,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am snappy_pale_cobra",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-22T20:31:11.000Z",
    "last_modified": "2025-10-24T18:17:16.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/pawankumar20/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-snappy_pale_cobra",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "rahmankhan2232/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-barky_jagged_okapi",
    "author": "unknown",
    "title": "rahmankhan2232/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-barky_jagged_okapi",
    "downloads": 128,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am barky_jagged_okapi",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-23T10:04:13.000Z",
    "last_modified": "2025-10-24T18:17:12.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/rahmankhan2232/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-barky_jagged_okapi",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "Elcasa/elcraige",
    "author": "unknown",
    "title": "Elcasa/elcraige",
    "downloads": 7570,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T00:35:34.000Z",
    "last_modified": "2025-10-24T18:16:51.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Elcasa/elcraige",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "elafah/wolf",
    "author": "unknown",
    "title": "elafah/wolf",
    "downloads": 9004,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T01:21:50.000Z",
    "last_modified": "2025-10-24T18:16:30.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/elafah/wolf",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "Marcus-KO/ModernBERT-distil-clinc-oos",
    "author": "unknown",
    "title": "Marcus-KO/ModernBERT-distil-clinc-oos",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "tensorboard",
      "safetensors",
      "modernbert",
      "text-classification",
      "generated_from_trainer",
      "base_model:answerdotai/ModernBERT-base",
      "base_model:finetune:answerdotai/ModernBERT-base",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-classification",
    "library": "transformers",
    "created_at": "2025-10-24T15:39:16.000Z",
    "last_modified": "2025-10-24T18:15:49.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Marcus-KO/ModernBERT-distil-clinc-oos",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "author": "unknown",
    "title": "rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "downloads": 359,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am sniffing_aquatic_badger",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-23T09:38:02.000Z",
    "last_modified": "2025-10-24T18:15:21.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/rajukhan8988/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-sniffing_aquatic_badger",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "kamakhan25/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-smooth_nasty_boar",
    "author": "unknown",
    "title": "kamakhan25/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-smooth_nasty_boar",
    "downloads": 480,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am smooth_nasty_boar",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-22T19:57:01.000Z",
    "last_modified": "2025-10-24T18:15:13.000Z",
    "days_since_creation": 1,
    "url": "https://huggingface.co/kamakhan25/Qwen2.5-0.5B-Instruct-Gensyn-Swarm-smooth_nasty_boar",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "3SixtyVentures/SN360383",
    "author": "unknown",
    "title": "3SixtyVentures/SN360383",
    "downloads": 9205,
    "likes": 0,
    "tags": [
      "safetensors",
      "llama",
      "region:us"
    ],
    "pipeline_tag": "",
    "library": "",
    "created_at": "2025-10-24T03:19:18.000Z",
    "last_modified": "2025-10-24T18:15:10.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/3SixtyVentures/SN360383",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "Dacryt/Qwen3-0.6B-Gensyn-Swarm-small_lazy_tuna",
    "author": "unknown",
    "title": "Dacryt/Qwen3-0.6B-Gensyn-Swarm-small_lazy_tuna",
    "downloads": 12,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "rl-swarm",
      "genrl-swarm",
      "grpo",
      "gensyn",
      "I am small_lazy_tuna",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2025-10-23T19:13:57.000Z",
    "last_modified": "2025-10-24T18:14:57.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Dacryt/Qwen3-0.6B-Gensyn-Swarm-small_lazy_tuna",
    "date": "2025-10-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  }
]